{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:18.928941Z",
     "iopub.status.busy": "2022-10-26T21:21:18.928534Z",
     "iopub.status.idle": "2022-10-26T21:21:29.775931Z",
     "shell.execute_reply": "2022-10-26T21:21:29.774978Z",
     "shell.execute_reply.started": "2022-10-26T21:21:18.928858Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import os\n",
    "import glob as gb\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM , Flatten , CuDNNLSTM , GRU, Bidirectional, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.callbacks import  CSVLogger\n",
    "\n",
    "\n",
    "# Model Evaluations\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:29.778267Z",
     "iopub.status.busy": "2022-10-26T21:21:29.777641Z",
     "iopub.status.idle": "2022-10-26T21:21:29.785861Z",
     "shell.execute_reply": "2022-10-26T21:21:29.784255Z",
     "shell.execute_reply.started": "2022-10-26T21:21:29.778231Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_file(filepath,i):\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    imu = data['imu']['data']\n",
    "    emg = data['emg']['data']\n",
    "    emg = np.array(emg)\n",
    "    imu = np.array(imu)\n",
    "    imu_gyr = np.array([(e['gyroscope']) for e in imu])\n",
    "    imu_acc = np.array([(e['acceleration']) for e in imu])\n",
    "    imu_orn = np.array([(e['orientation']) for e in imu])\n",
    "    #timestamp = [i]\n",
    "    #timestamp = np.repeat(timestamp, 400, axis=None)\n",
    "    #timestamp= timestamp.reshape(400,1)\n",
    "    dataset = tf.concat([emg, imu_gyr, imu_acc, imu_orn], axis=1, name='concat')\n",
    "    dataset = np.array(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:29.787776Z",
     "iopub.status.busy": "2022-10-26T21:21:29.787052Z",
     "iopub.status.idle": "2022-10-26T21:21:29.804702Z",
     "shell.execute_reply": "2022-10-26T21:21:29.803654Z",
     "shell.execute_reply.started": "2022-10-26T21:21:29.787710Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../input/emgimu/An-EMG-and-IMU-Dataset-for-the-Italian-Sign-Language-Alphabet-master/Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:29.808091Z",
     "iopub.status.busy": "2022-10-26T21:21:29.807698Z",
     "iopub.status.idle": "2022-10-26T21:21:42.871102Z",
     "shell.execute_reply": "2022-10-26T21:21:42.868816Z",
     "shell.execute_reply.started": "2022-10-26T21:21:29.808057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 21:21:29.975328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:29.976270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.363479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.364313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.365067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.365878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.369797: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 21:21:30.625406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.626227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.626980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.627700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.628394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:30.629114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.424378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.425322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.426051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.426853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.427620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.428316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-10-26 21:21:35.434101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 21:21:35.434785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 780 items in X \n"
     ]
    }
   ],
   "source": [
    "# For Files Data\n",
    "X = []\n",
    "y = []\n",
    "i=0\n",
    "for folder in  os.listdir(data_path) : \n",
    "    j=0\n",
    "    files = gb.glob(pathname= str( data_path  + folder + '/*.json'))\n",
    "    for file in files: \n",
    "        data = load_file(file,j)\n",
    "        X.append(data)\n",
    "        y.append(i)\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "print(f'we have {len(X)} items in X ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:42.874993Z",
     "iopub.status.busy": "2022-10-26T21:21:42.874670Z",
     "iopub.status.idle": "2022-10-26T21:21:42.899236Z",
     "shell.execute_reply": "2022-10-26T21:21:42.898251Z",
     "shell.execute_reply.started": "2022-10-26T21:21:42.874962Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:42.901168Z",
     "iopub.status.busy": "2022-10-26T21:21:42.900687Z",
     "iopub.status.idle": "2022-10-26T21:21:42.909098Z",
     "shell.execute_reply": "2022-10-26T21:21:42.907671Z",
     "shell.execute_reply.started": "2022-10-26T21:21:42.901131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is :  (780, 400, 18)\n",
      "y shape is :  (780,)\n"
     ]
    }
   ],
   "source": [
    "print('X shape is : ' , X.shape)\n",
    "print('y shape is : ' , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:42.910970Z",
     "iopub.status.busy": "2022-10-26T21:21:42.910572Z",
     "iopub.status.idle": "2022-10-26T21:21:42.922703Z",
     "shell.execute_reply": "2022-10-26T21:21:42.921650Z",
     "shell.execute_reply.started": "2022-10-26T21:21:42.910943Z"
    }
   },
   "outputs": [],
   "source": [
    "mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, \n",
    "           'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, \n",
    "           'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:42.924791Z",
     "iopub.status.busy": "2022-10-26T21:21:42.924443Z",
     "iopub.status.idle": "2022-10-26T21:21:42.935666Z",
     "shell.execute_reply": "2022-10-26T21:21:42.934634Z",
     "shell.execute_reply.started": "2022-10-26T21:21:42.924757Z"
    }
   },
   "outputs": [],
   "source": [
    "def getcode(n) : \n",
    "    for x , y in mapping.items() : \n",
    "        if n == y : \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:42.938689Z",
     "iopub.status.busy": "2022-10-26T21:21:42.937791Z",
     "iopub.status.idle": "2022-10-26T21:21:43.153434Z",
     "shell.execute_reply": "2022-10-26T21:21:43.152363Z",
     "shell.execute_reply.started": "2022-10-26T21:21:42.938651Z"
    }
   },
   "outputs": [],
   "source": [
    "new_X = []\n",
    "new_y = []\n",
    "for j in range(780):\n",
    "    for i in range(400):\n",
    "        new_X.append(X[j][i])\n",
    "        new_y.append(y[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:43.157799Z",
     "iopub.status.busy": "2022-10-26T21:21:43.157176Z",
     "iopub.status.idle": "2022-10-26T21:21:43.164374Z",
     "shell.execute_reply": "2022-10-26T21:21:43.163202Z",
     "shell.execute_reply.started": "2022-10-26T21:21:43.157762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312000\n",
      "312000\n"
     ]
    }
   ],
   "source": [
    "print(len(new_X))\n",
    "print(len(new_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:43.166807Z",
     "iopub.status.busy": "2022-10-26T21:21:43.166093Z",
     "iopub.status.idle": "2022-10-26T21:21:43.306559Z",
     "shell.execute_reply": "2022-10-26T21:21:43.305541Z",
     "shell.execute_reply.started": "2022-10-26T21:21:43.166774Z"
    }
   },
   "outputs": [],
   "source": [
    "new_X = np.array(new_X)\n",
    "new_y = np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:43.310490Z",
     "iopub.status.busy": "2022-10-26T21:21:43.310179Z",
     "iopub.status.idle": "2022-10-26T21:21:43.364174Z",
     "shell.execute_reply": "2022-10-26T21:21:43.363196Z",
     "shell.execute_reply.started": "2022-10-26T21:21:43.310462Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "new_X, new_y = shuffle(new_X, new_y, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:43.366323Z",
     "iopub.status.busy": "2022-10-26T21:21:43.365657Z",
     "iopub.status.idle": "2022-10-26T21:21:43.372421Z",
     "shell.execute_reply": "2022-10-26T21:21:43.371231Z",
     "shell.execute_reply.started": "2022-10-26T21:21:43.366285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_X shape is :  (312000, 18)\n",
      "new_y shape is :  (312000,)\n"
     ]
    }
   ],
   "source": [
    "print('new_X shape is : ' , new_X.shape)\n",
    "print('new_y shape is : ' , new_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model with samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:43.374637Z",
     "iopub.status.busy": "2022-10-26T21:21:43.373929Z",
     "iopub.status.idle": "2022-10-26T21:21:43.407918Z",
     "shell.execute_reply": "2022-10-26T21:21:43.406978Z",
     "shell.execute_reply.started": "2022-10-26T21:21:43.374600Z"
    }
   },
   "outputs": [],
   "source": [
    "new_X = new_X.reshape(len(new_X), new_X.shape[1],1)\n",
    "new_y = to_categorical(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T21:21:43.409877Z",
     "iopub.status.busy": "2022-10-26T21:21:43.409501Z",
     "iopub.status.idle": "2022-10-26T21:21:43.415571Z",
     "shell.execute_reply": "2022-10-26T21:21:43.413675Z",
     "shell.execute_reply.started": "2022-10-26T21:21:43.409842Z"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T00:01:05.084297Z",
     "iopub.status.busy": "2022-10-27T00:01:05.083923Z",
     "iopub.status.idle": "2022-10-27T01:32:53.223290Z",
     "shell.execute_reply": "2022-10-27T01:32:53.222263Z",
     "shell.execute_reply.started": "2022-10-27T00:01:05.084265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 59s 8ms/step - loss: 2.5598 - accuracy: 0.2133\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3726 - accuracy: 0.2622\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3125 - accuracy: 0.2790\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2777 - accuracy: 0.2894\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2531 - accuracy: 0.2965\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2352 - accuracy: 0.3011\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2198 - accuracy: 0.3057\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2060 - accuracy: 0.3097\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1946 - accuracy: 0.3118\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1854 - accuracy: 0.3153\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1752 - accuracy: 0.3180\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1659 - accuracy: 0.3202\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1578 - accuracy: 0.3233\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1508 - accuracy: 0.3253\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1424 - accuracy: 0.3265\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1367 - accuracy: 0.3283\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1294 - accuracy: 0.3305\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1217 - accuracy: 0.3332\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1174 - accuracy: 0.3338\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1109 - accuracy: 0.3355\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1058 - accuracy: 0.3374\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1008 - accuracy: 0.3392\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0941 - accuracy: 0.3396\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0890 - accuracy: 0.3423\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0845 - accuracy: 0.3430\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0796 - accuracy: 0.3440\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0749 - accuracy: 0.3448\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0709 - accuracy: 0.3465\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0691 - accuracy: 0.3474\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0634 - accuracy: 0.3486\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0593 - accuracy: 0.3496\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0530 - accuracy: 0.3517\n",
      "Score for fold 1: loss of 2.1793060302734375;     accuracy of 32.40705132484436%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 8ms/step - loss: 2.5565 - accuracy: 0.2137\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3653 - accuracy: 0.2649\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3096 - accuracy: 0.2813\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2770 - accuracy: 0.2900\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2533 - accuracy: 0.2961\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2333 - accuracy: 0.3023\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2201 - accuracy: 0.3059\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2066 - accuracy: 0.3101\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1954 - accuracy: 0.3117\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1847 - accuracy: 0.3153\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1743 - accuracy: 0.3184\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1645 - accuracy: 0.3203\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1582 - accuracy: 0.3228\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1494 - accuracy: 0.3251\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1417 - accuracy: 0.3280\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1341 - accuracy: 0.3300\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1272 - accuracy: 0.3310\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1217 - accuracy: 0.3332\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1158 - accuracy: 0.3354\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1099 - accuracy: 0.3357\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1042 - accuracy: 0.3392\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0984 - accuracy: 0.3399\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0922 - accuracy: 0.3416\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0870 - accuracy: 0.3427\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0834 - accuracy: 0.3443\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0776 - accuracy: 0.3458\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0732 - accuracy: 0.3469\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0703 - accuracy: 0.3473\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0645 - accuracy: 0.3494\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0607 - accuracy: 0.3503\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0582 - accuracy: 0.3507\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0531 - accuracy: 0.3520\n",
      "Score for fold 2: loss of 2.1859073638916016;     accuracy of 31.987178325653076%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 8ms/step - loss: 2.5503 - accuracy: 0.2155\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3613 - accuracy: 0.2646\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3012 - accuracy: 0.2832\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2708 - accuracy: 0.2915\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2459 - accuracy: 0.2989\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2275 - accuracy: 0.3045\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2121 - accuracy: 0.3080\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1992 - accuracy: 0.3127\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1889 - accuracy: 0.3154\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1784 - accuracy: 0.3182\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1696 - accuracy: 0.3206\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1585 - accuracy: 0.3234\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1512 - accuracy: 0.3249\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1440 - accuracy: 0.3272\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1363 - accuracy: 0.3288\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1307 - accuracy: 0.3297\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1220 - accuracy: 0.3334\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1164 - accuracy: 0.3343\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1101 - accuracy: 0.3366\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1032 - accuracy: 0.3383\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0988 - accuracy: 0.3388\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0922 - accuracy: 0.3417\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0863 - accuracy: 0.3438\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0808 - accuracy: 0.3447\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0781 - accuracy: 0.3459\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0727 - accuracy: 0.3470\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0690 - accuracy: 0.3483\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0626 - accuracy: 0.3495\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0595 - accuracy: 0.3511\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0553 - accuracy: 0.3509\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0528 - accuracy: 0.3520\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0489 - accuracy: 0.3530\n",
      "Score for fold 3: loss of 2.1889734268188477;     accuracy of 31.7724347114563%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 8ms/step - loss: 2.5537 - accuracy: 0.2144\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3655 - accuracy: 0.2653\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3057 - accuracy: 0.2822\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2727 - accuracy: 0.2914\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2475 - accuracy: 0.2992\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2284 - accuracy: 0.3033\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2126 - accuracy: 0.3081\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2002 - accuracy: 0.3125\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1898 - accuracy: 0.3151\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1793 - accuracy: 0.3174\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1698 - accuracy: 0.3196\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1607 - accuracy: 0.3226\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1521 - accuracy: 0.3260\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1463 - accuracy: 0.3265\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1376 - accuracy: 0.3289\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1294 - accuracy: 0.3322\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1214 - accuracy: 0.3340\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1170 - accuracy: 0.3354\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1092 - accuracy: 0.3382\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1044 - accuracy: 0.3375\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0984 - accuracy: 0.3407\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0931 - accuracy: 0.3410\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0875 - accuracy: 0.3429\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0829 - accuracy: 0.3448\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0782 - accuracy: 0.3456\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0745 - accuracy: 0.3469\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0712 - accuracy: 0.3480\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0658 - accuracy: 0.3503\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0611 - accuracy: 0.3508\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0577 - accuracy: 0.3521\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0550 - accuracy: 0.3520\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0492 - accuracy: 0.3544\n",
      "Score for fold 4: loss of 2.206974744796753;     accuracy of 31.39423131942749%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 8ms/step - loss: 2.5604 - accuracy: 0.2130\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3665 - accuracy: 0.2649\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3085 - accuracy: 0.2809\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2728 - accuracy: 0.2918\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2481 - accuracy: 0.2980\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2297 - accuracy: 0.3037\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2152 - accuracy: 0.3071\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2013 - accuracy: 0.3111\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1914 - accuracy: 0.3140\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1799 - accuracy: 0.3173\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1700 - accuracy: 0.3214\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1617 - accuracy: 0.3218\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1539 - accuracy: 0.3251\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1464 - accuracy: 0.3271\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1405 - accuracy: 0.3292\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1340 - accuracy: 0.3304\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1254 - accuracy: 0.3320\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1193 - accuracy: 0.3350\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1133 - accuracy: 0.3364\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1080 - accuracy: 0.3373\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1021 - accuracy: 0.3385\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0968 - accuracy: 0.3397\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0914 - accuracy: 0.3419\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0872 - accuracy: 0.3429\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0808 - accuracy: 0.3441\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0779 - accuracy: 0.3451\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0722 - accuracy: 0.3474\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0671 - accuracy: 0.3490\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0635 - accuracy: 0.3495\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0587 - accuracy: 0.3505\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0549 - accuracy: 0.3515\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0517 - accuracy: 0.3522\n",
      "Score for fold 5: loss of 2.1707005500793457;     accuracy of 32.83974230289459%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 18s 7ms/step - loss: 2.5547 - accuracy: 0.2150\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3622 - accuracy: 0.2668\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3032 - accuracy: 0.2824\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2685 - accuracy: 0.2928\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2449 - accuracy: 0.2983\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2273 - accuracy: 0.3042\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2111 - accuracy: 0.3081\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1992 - accuracy: 0.3119\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1877 - accuracy: 0.3146\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1766 - accuracy: 0.3175\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1668 - accuracy: 0.3207\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1578 - accuracy: 0.3240\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1498 - accuracy: 0.3249\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1439 - accuracy: 0.3273\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1332 - accuracy: 0.3301\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1287 - accuracy: 0.3312\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1204 - accuracy: 0.3337\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1145 - accuracy: 0.3350\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1080 - accuracy: 0.3369\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1028 - accuracy: 0.3381\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0973 - accuracy: 0.3396\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0903 - accuracy: 0.3411\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0863 - accuracy: 0.3439\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0798 - accuracy: 0.3446\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0759 - accuracy: 0.3453\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0705 - accuracy: 0.3463\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0636 - accuracy: 0.3502\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0611 - accuracy: 0.3501\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0563 - accuracy: 0.3511\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0530 - accuracy: 0.3519\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0486 - accuracy: 0.3524\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0454 - accuracy: 0.3537\n",
      "Score for fold 6: loss of 2.204838275909424;     accuracy of 31.74038529396057%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 7ms/step - loss: 2.5580 - accuracy: 0.2142\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3618 - accuracy: 0.2653\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3046 - accuracy: 0.2823\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2694 - accuracy: 0.2924\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2469 - accuracy: 0.2990\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2272 - accuracy: 0.3038\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2125 - accuracy: 0.3084\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1998 - accuracy: 0.3114\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1875 - accuracy: 0.3152\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1785 - accuracy: 0.3172\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1686 - accuracy: 0.3199\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1577 - accuracy: 0.3228\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1508 - accuracy: 0.3247\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1427 - accuracy: 0.3266\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1346 - accuracy: 0.3291\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1271 - accuracy: 0.3321\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1209 - accuracy: 0.3333\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1162 - accuracy: 0.3348\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1098 - accuracy: 0.3365\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1013 - accuracy: 0.3391\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.0949 - accuracy: 0.3401\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0905 - accuracy: 0.3421\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0871 - accuracy: 0.3413\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0804 - accuracy: 0.3438\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0784 - accuracy: 0.3446\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0728 - accuracy: 0.3466\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0680 - accuracy: 0.3475\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0637 - accuracy: 0.3491\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0596 - accuracy: 0.3502\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0550 - accuracy: 0.3508\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.0512 - accuracy: 0.3519\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0477 - accuracy: 0.3525\n",
      "Score for fold 7: loss of 2.17988657951355;     accuracy of 32.47756361961365%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 8ms/step - loss: 2.5548 - accuracy: 0.2145\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3644 - accuracy: 0.2645\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3058 - accuracy: 0.2815\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2706 - accuracy: 0.2911\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2483 - accuracy: 0.2981\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2304 - accuracy: 0.3027\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2172 - accuracy: 0.3061\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2053 - accuracy: 0.3107\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1930 - accuracy: 0.3131\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1821 - accuracy: 0.3165\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1729 - accuracy: 0.3193\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1644 - accuracy: 0.3212\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1562 - accuracy: 0.3235\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 18s 8ms/step - loss: 2.1488 - accuracy: 0.3253\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1419 - accuracy: 0.3278\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1375 - accuracy: 0.3283\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1312 - accuracy: 0.3302\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1236 - accuracy: 0.3329\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1192 - accuracy: 0.3331\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1111 - accuracy: 0.3355\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1069 - accuracy: 0.3374\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1003 - accuracy: 0.3386\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0947 - accuracy: 0.3408\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0922 - accuracy: 0.3414\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0877 - accuracy: 0.3431\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0831 - accuracy: 0.3438\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0790 - accuracy: 0.3451\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0743 - accuracy: 0.3460\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0697 - accuracy: 0.3470\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0650 - accuracy: 0.3481\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0625 - accuracy: 0.3490\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0585 - accuracy: 0.3512\n",
      "Score for fold 8: loss of 2.194920778274536;     accuracy of 31.958332657814026%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 18s 7ms/step - loss: 2.5588 - accuracy: 0.2124\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3678 - accuracy: 0.2645\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3077 - accuracy: 0.2803\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2738 - accuracy: 0.2914\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2502 - accuracy: 0.2966\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2325 - accuracy: 0.3021\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2175 - accuracy: 0.3060\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2059 - accuracy: 0.3096\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1933 - accuracy: 0.3131\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1834 - accuracy: 0.3161\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1739 - accuracy: 0.3192\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1634 - accuracy: 0.3206\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1550 - accuracy: 0.3239\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1471 - accuracy: 0.3259\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1404 - accuracy: 0.3281\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1340 - accuracy: 0.3293\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1267 - accuracy: 0.3304\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1194 - accuracy: 0.3338\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1129 - accuracy: 0.3356\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1093 - accuracy: 0.3370\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1029 - accuracy: 0.3384\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0973 - accuracy: 0.3406\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0916 - accuracy: 0.3416\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0872 - accuracy: 0.3425\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0809 - accuracy: 0.3448\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0765 - accuracy: 0.3452\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0716 - accuracy: 0.3468\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0679 - accuracy: 0.3492\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.0633 - accuracy: 0.3487\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0590 - accuracy: 0.3501\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0544 - accuracy: 0.3524\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0511 - accuracy: 0.3521\n",
      "Score for fold 9: loss of 2.191990852355957;     accuracy of 31.948718428611755%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/32\n",
      "2194/2194 [==============================] - 19s 7ms/step - loss: 2.5542 - accuracy: 0.2149\n",
      "Epoch 2/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.3644 - accuracy: 0.2644\n",
      "Epoch 3/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.3068 - accuracy: 0.2809\n",
      "Epoch 4/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2719 - accuracy: 0.2916\n",
      "Epoch 5/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2476 - accuracy: 0.2985\n",
      "Epoch 6/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2291 - accuracy: 0.3036\n",
      "Epoch 7/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.2142 - accuracy: 0.3081\n",
      "Epoch 8/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.2001 - accuracy: 0.3125\n",
      "Epoch 9/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1880 - accuracy: 0.3147\n",
      "Epoch 10/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1772 - accuracy: 0.3179\n",
      "Epoch 11/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1683 - accuracy: 0.3195\n",
      "Epoch 12/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1611 - accuracy: 0.3230\n",
      "Epoch 13/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1533 - accuracy: 0.3248\n",
      "Epoch 14/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1447 - accuracy: 0.3266\n",
      "Epoch 15/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1367 - accuracy: 0.3292\n",
      "Epoch 16/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1309 - accuracy: 0.3305\n",
      "Epoch 17/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1243 - accuracy: 0.3324\n",
      "Epoch 18/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.1187 - accuracy: 0.3336\n",
      "Epoch 19/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1121 - accuracy: 0.3354\n",
      "Epoch 20/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.1056 - accuracy: 0.3374\n",
      "Epoch 21/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.1015 - accuracy: 0.3383\n",
      "Epoch 22/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0970 - accuracy: 0.3403\n",
      "Epoch 23/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0892 - accuracy: 0.3418\n",
      "Epoch 24/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0849 - accuracy: 0.3441\n",
      "Epoch 25/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.0818 - accuracy: 0.3438\n",
      "Epoch 26/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0761 - accuracy: 0.3458\n",
      "Epoch 27/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0716 - accuracy: 0.3463\n",
      "Epoch 28/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0683 - accuracy: 0.3484\n",
      "Epoch 29/32\n",
      "2194/2194 [==============================] - 17s 8ms/step - loss: 2.0637 - accuracy: 0.3489\n",
      "Epoch 30/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0595 - accuracy: 0.3497\n",
      "Epoch 31/32\n",
      "2194/2194 [==============================] - 16s 8ms/step - loss: 2.0552 - accuracy: 0.3517\n",
      "Epoch 32/32\n",
      "2194/2194 [==============================] - 16s 7ms/step - loss: 2.0518 - accuracy: 0.3525\n",
      "Score for fold 10: loss of 2.177428960800171;     accuracy of 32.1794867515564%\n"
     ]
    }
   ],
   "source": [
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(new_X, new_y):\n",
    "\n",
    "  # Model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, 3, activation='relu', input_shape=(18, 1))),\n",
    "    model.add(MaxPooling1D(3)),\n",
    "    model.add(Conv1D(64, 3, activation='relu')),\n",
    "    model.add(MaxPooling1D(3)),\n",
    "    model.add(Bidirectional(CuDNNLSTM(units=265, return_sequences=True)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Bidirectional(CuDNNLSTM(units=128, return_sequences=True)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(26,activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#     es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "#     checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"./checkpoint-{epoch:02d}.hdf5\",\n",
    "#                                                       verbose=1, save_best_only=True,\n",
    "#                                                       monitor='val_acc',mode='max')\n",
    "#     csv_logger = CSVLogger('training_set_iranalysis3.csv',separator=',', append=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    \n",
    "    history = model.fit(new_X[train], new_y[train], \n",
    "                        batch_size=128, epochs=32)\n",
    "\n",
    "    \n",
    "    scores = model.evaluate(new_X[test], new_y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; \\\n",
    "    {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    \n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T01:32:53.226005Z",
     "iopub.status.busy": "2022-10-27T01:32:53.225135Z",
     "iopub.status.idle": "2022-10-27T01:32:53.234242Z",
     "shell.execute_reply": "2022-10-27T01:32:53.233263Z",
     "shell.execute_reply.started": "2022-10-27T01:32:53.225965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 2.1793060302734375 - Accuracy: 32.40705132484436%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.1859073638916016 - Accuracy: 31.987178325653076%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 2.1889734268188477 - Accuracy: 31.7724347114563%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 2.206974744796753 - Accuracy: 31.39423131942749%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.1707005500793457 - Accuracy: 32.83974230289459%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 2.204838275909424 - Accuracy: 31.74038529396057%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 2.17988657951355 - Accuracy: 32.47756361961365%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 2.194920778274536 - Accuracy: 31.958332657814026%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 2.191990852355957 - Accuracy: 31.948718428611755%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 2.177428960800171 - Accuracy: 32.1794867515564%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 32.07051247358322 (+- 0.396496172060866)\n",
      "> Loss: 2.188092756271362\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
